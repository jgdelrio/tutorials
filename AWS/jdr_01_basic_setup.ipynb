{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable access to Amazon services like S3:\n",
    "- get_execution_role: allow Amazon SageMaker to assume the role created during instance creation and accesses resources on your behalf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a bucket\n",
    "This will hosts the dataset that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'dataen-sagemaker-dev/datasets/cal-tech' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the containers\n",
    "Containers (docker containers) as the training job defined in this notebook will run in the container for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/image-classification:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/image-classification:latest'}\n",
    "training_image = containers[boto3.Session().region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import boto3\n",
    "\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "def upload_to_s3(bucket, file, channel):\n",
    "    \"\"\"\n",
    "    Stores the file in the 'channel' folder within the specified bucket\n",
    "    If bucket contains folders in the path that is \n",
    "    \"\"\"\n",
    "    subfolder = None\n",
    "    if \"/\" in bucket:\n",
    "        buc_folders = bucket.split('/')\n",
    "        bucket = buc_folders[0]\n",
    "        subfolder = buc_folders[1:]\n",
    "    if isinstance(bucket, list):\n",
    "        bucket, subfolder = bucket[0], bucket[1:]\n",
    "        \n",
    "    if subfolder:\n",
    "        key = \"/\".join(subfolder) + \"/\" + channel + \"/\" + file\n",
    "    else:\n",
    "        key = channel + \"/\" + file\n",
    "    \n",
    "    # Read file\n",
    "    with open(file, \"rb\") as data:\n",
    "        s3 = boto3.resource('s3')\n",
    "        s3.Bucket(bucket).put_object(Key=key, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caltech-256\n",
    "\n",
    "# download('http://data.mxnet.io/data/caltech-256/caltech-256-60-train.rec')\n",
    "# upload_to_s3('train', 'caltech-256-60-train.rec', bucket)\n",
    "# download('http://data.mxnet.io/data/caltech-256/caltech-256-60-val.rec')\n",
    "# upload_to_s3('validation', 'caltech-256-60-val.rec', bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model in SageMaker\n",
    "\n",
    "We need to create a training job. A training job includes the following information:\n",
    "\n",
    "- S3 Bucket (input data) with the training data.\n",
    "- S3 Bucket (output) for the model, data, etc.\n",
    "- Compute resources to use in SageMaker (ML compute instances managed by Amazon SageMaker).\n",
    "- ECR (Elastic Container Registry) path where the training code is stored.\n",
    "\n",
    "For example we can pass the image classifier (ResNet) built in Amazon SageMaker. The checkpoint_frequency determines the frequency by which model files are stored during training. If we only need the final model file, we set it equal to the number of epochs.\n",
    "\n",
    "Please make a note of **job_name_prefix, S3OutputPath, InstanceType, InstanceCount**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Training Parameters\n",
    "\n",
    "buc_folders = bucket.split('/')\n",
    "if len(buc_folders) > 1:\n",
    "    bucket, subfolders = buc_folders[0], \"/\".join(buc_folders[1:])\n",
    "else:\n",
    "    subfolders = \"datasets/cal-tech\"\n",
    "\n",
    "# The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200\n",
    "num_layers = \"18\" \n",
    "# we need to specify the input image shape for the training data\n",
    "image_shape = \"3,224,224\"\n",
    "# we also need to specify the number of training samples in the training set\n",
    "# for caltech it is 15420\n",
    "num_training_samples = \"15420\"\n",
    "# specify the number of output classes\n",
    "num_classes = \"257\"\n",
    "# batch size for training\n",
    "mini_batch_size =  \"128\"\n",
    "# number of epochs\n",
    "epochs = \"3\"\n",
    "# learning rate\n",
    "learning_rate = \"0.1\"\n",
    "#optimizer\n",
    "optimizer ='Adam'\n",
    "#checkpoint_frequency\n",
    "checkpoint_frequency = epochs     # We just want the final model so equal to epochs\n",
    "#scheduler_step\n",
    "lr_scheduler_step=\"30,90,180\"\n",
    "#scheduler_factor\n",
    "lr_scheduler_factor=\"0.1\"\n",
    "#augmentation_type\n",
    "augmentation_type=\"crop_color_transform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack parameters\n",
    "\n",
    "Get all parameters into a dictionary structure: Job name, buckets, instance...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ImageClassificationResNet-2019-10-31T08-45-28\n",
      "\n",
      "Input Data Location: \n",
      "  {'S3DataType': 'S3Prefix', 'S3Uri': 's3://dataen-sagemaker-dev/train/', 'S3DataDistributionType': 'FullyReplicated'}\n",
      "\n",
      "Output Location: s3://dataen-sagemaker-dev/datasets/cal-tech\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# create unique job name \n",
    "job_name_prefix = 'ImageClassificationResNet'\n",
    "timestamp = time.strftime('-%Y-%m-%dT%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_params = \\\n",
    "{\n",
    "    # specify the training docker image\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": f\"s3://{bucket}/{subfolders}\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.8xlarge\",\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"image_shape\": image_shape,\n",
    "        \"num_layers\": str(num_layers),\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "        \"num_classes\": str(num_classes),\n",
    "        \"mini_batch_size\": str(mini_batch_size),\n",
    "        \"epochs\": str(epochs),\n",
    "        \"learning_rate\": str(learning_rate),\n",
    "        \"lr_scheduler_step\": str(lr_scheduler_step),\n",
    "        \"lr_scheduler_factor\": str(lr_scheduler_factor),\n",
    "        \"augmentation_type\": str(augmentation_type),\n",
    "        \"checkpoint_frequency\": str(checkpoint_frequency),\n",
    "        \"augmentation_type\" : str(augmentation_type)\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 360000\n",
    "    },\n",
    "#Training data should be inside a subdirectory called \"train\"\n",
    "#Validation data should be inside a subdirectory called \"validation\"\n",
    "#The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f's3://{bucket}/train/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": f's3://{bucket}/validation/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print(f'Training job name: {job_name}')\n",
    "print(f\"\\nInput Data Location: \\n  {training_params['InputDataConfig'][0]['DataSource']['S3DataSource']}\")\n",
    "print(f\"\\nOutput Location: {training_params['OutputDataConfig']['S3OutputPath']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the job and check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job current status: InProgress\n",
      "Training job ended with status: Completed\n"
     ]
    }
   ],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print(f'Training job current status: {status}')\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print(f'Training failed with the following error: {message}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda service for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import greengrasssdk\n",
    "from threading import Timer\n",
    "import time\n",
    "import awscam\n",
    "import cv2\n",
    "import mo\n",
    "from threading import Thread\n",
    "\n",
    "# Creating a greengrass core sdk client\n",
    "client = greengrasssdk.client('iot-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More at [\"build your own object classification model\"](https://aws.amazon.com/blogs/machine-learning/build-your-own-object-classification-model-in-sagemaker-and-import-it-to-deeplens/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
