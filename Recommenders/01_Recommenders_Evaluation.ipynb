{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders Evaluation\n",
    "\n",
    "Previously we have seen the basics of Recommender Systems and we have created a variety of models (Most Popular, Content Recommender, Collaborative Filtering Models...).\n",
    "\n",
    "Now we are going to characterize some recommenders to understand their differences. To do that we need some metrics to help to gather different information and stablish a common ground to compare the models.\n",
    "\n",
    "The main motivation of the Recommenders is to help the user to find relevant items (books, films, news, products, etc) and to do so they try to use the knowledge gather about the user through his/her history and habits. That's how many Recommenders work (Collaborative Filtering, ALS, Deep Learning models...) but there are scenarios where this stragegy will fail miserably. That is when we don't have any history about a user.\n",
    "\n",
    "When we don't have any pre-knowledge of a user (which is known as *Cold Star*), we apply other strategies like *\"Most Popular\"*.\n",
    "\n",
    "The result is that we up end with multiple Recommender Engines to account for different strategies and we must combine them to cope with the different scenarios, that is using the best tool for each task.\n",
    "\n",
    "Depending on the model, we tackle one or more of the following points:\n",
    "- Provide relevant recommendations to the user at that moment in time\n",
    "- Cold start\n",
    "- Provide enough novelty in the suggestions\n",
    "\n",
    "\n",
    "## Metrics:\n",
    "\n",
    "In Recommender Systems we could still apply the standard and basic metrics but those will miss in very valuable information about how we rank and sort the recommendations. For that reason we find some specialized metrics. Overall we can classify the metrics as:\n",
    "\n",
    "1. **Basic metrics** like RMSE or MAE can measure the deviation in the predictions and are very quick to run. The downside typically is that the model tends to achieve a good fit for the items with the most ratings but items with few ratings don't account for much in therms of their impact on the loss function. As a result predictions for items with few ratings will be off spoiling the results.\n",
    "2. **Specialized metrics** that focus purely on recommendation systems. In this category we have metrics like **Mean Average Precision (MAP)** and **Normalized Discounted Cumulative Gain (NDCG)**.\n",
    "3. **Description metrics** include metrics that help to characterize the model. For example *item coverage*, calculating how many items are we recommending from the list of all posible items in our database. Or *coverage by year* to measure if we recommend only articles from this year or we are distributing the results across a wider range. Other possibilities that may be relevant depending on the sector include by genre, by author, by manufacturer, by size, by color, etc...\n",
    "\n",
    "In many recommendation metrics we use <metric@k>, for example MAP@k or NDCG@k.<br>\n",
    "This means that for every user we ask to get only **k** recommendations and it is based on the accuracy of those k recommendations in which we want to characterize the model and improve the model. The logic behind is that most of the time we only care about the first 5 or 10 recommendations for a user as he is unlikely to explore all the options available, or our interface cannot provide more recommendations in a reasonable way.<br>\n",
    "So @k is a cutoff that we apply to make sure that our models focus on improving the recommendations that really matter, the ones that the user is going to be exposed to.\n",
    "\n",
    "### **Mean Average Precision**\n",
    "MAP is just an average of APs (average precision) for all users in the evaluation. So if we have 1k users, we sum APs for each user and divide the sum by 1k. As mentioned, we will use MAP@k\n",
    "\n",
    "Important factors to remember:\n",
    "* We want to recommend at most k items for each user.\n",
    "* It is worth to submit all k recommendations because we are not penalized for bad guesses.\n",
    "* Order matters, so it’s better to submit more certain recommendations first followed by recommendations in which we have less confidence.\n",
    "\n",
    "<br><br>\n",
    "To understand MAP we do a quick recap of bynary classification metrics Precision and Recall:\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision:​​​​​​P​​=​​{\\frac{\\#​correct​positives}{\\#​predicted​positives}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall:​​​​​​R​​=​​{\\frac{\\#​correct​positives}{\\#​with​condition}}\n",
    "\\end{equation*}\n",
    "\n",
    "Also in different terminology:<br>\n",
    "* Precision = (1 - false positive rate)\n",
    "* Recall = (1 - false negative rate)\n",
    "\n",
    "These metrics translated to the specific application in Recommender Systems is:\n",
    "\n",
    "\\begin{equation*}\n",
    "Recommender​​Precision:​​​​​​P​​=​​{\\frac{\\#​of​our​relevant​recommendations}{\\#​of​items​that​we​recommend}}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Recommender​​Recall:​​​​​​R​​=​​{\\frac{\\#​of​our​relevant​recommendations}{\\#​of​all​possible​relevant​recommendations}}\n",
    "\\end{equation*}\n",
    "\n",
    "And to give an example, we are asked to recommend 5 items (k=5) but from the items we have only 3 relevant (m=3) to the user and my succesful predictions are ranked as [0, 1, 1, 0, 0]:<br>\n",
    "* \\# of items that we recommend is 5\n",
    "* \\# of our relevant recommendations is 2\n",
    "* \\# of all possible relevant recommendations is 3\n",
    "* **precision** is 2/5\n",
    "* **recall** is 2/3\n",
    "\n",
    "Now we apply the cutoff at k and the resulting metric that we obtain is:\n",
    "\\begin{equation*}\n",
    "AP@k​​=​​{\\frac{1}{m}}​\\sum_{k=1}^N​\\left(P(k)​if​k^{th}​item​was​relevant \\right)​​=​​{\\frac{1}{m}}​\\sum_{k=1}^N​P(k)​\\cdot​rel(k)\n",
    "\\end{equation*}\n",
    "\n",
    "With:\n",
    "\\begin{equation*}\n",
    "rel(k)​​=​​1​​​​--->​​is​relevant\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "rel(k)​​=​​0​​​​--->​not​relevant\n",
    "\\end{equation*}\n",
    "\n",
    "#### AP example\n",
    "\n",
    "Given N=3 recommendations (AP@3) for a user, depending on how relevant are the recommendations we can have:\n",
    "\n",
    "| Recommendations | Precision@k | AP@k |\n",
    "|:-----------:|:-----------:|:-----------:|\n",
    "|  [0,0,1]  |   [0,0,1/3]   | (1/3)[1/3] = 0.11 |\n",
    "|  [0,1,1]  |  [0,1/2,2/3]  | (1/3)[(1/2) + (2/3)] = 0.38 |\n",
    "|  [1,1,1]  | [1/1,2/2,3/3] | (1/3)[(1) + (2/2) + (3/3)] = 1 |\n",
    "\n",
    "So the more accurate recommendations I get the larger AP gets because we only sum the k<sup>th</sup> subset precision if the k<sup>th</sup> recommendation was correct so there is a heavy penalty when having incorrect recommendations as well as for not having the correct recommendations at the front.\n",
    "\n",
    "AP does not penalize having more recommendations but it is really worth to make sure you have the correct ones at the front.\n",
    "\n",
    "#### MAP@k\n",
    "Now accounting for all the possible users |U|, we create the average of the metric for each possible user. This can be translated as the average of all users of how many *relevant and retrieved* recommendations at k, divided by the total *number of relevant possible recommendations* also at k (sometimes expressed as *min(m, k)* with m being the number or relevant items found):\n",
    "\\begin{equation*}\n",
    "MAP@k​​=​​{\\frac{1}{|U|}}​\\sum_{u=1}^z​\\left( AP@k \\right)_u​​=​​{\\frac{1}{|U|}}​\\sum_{u=1}^z​{\\frac{1}{m}}​\\sum_{k=1}^N​P_u(k)​\\cdot​rel_u(k)\n",
    "\\end{equation*}\n",
    "\n",
    "<br><br>\n",
    "Some variations of this metric may try to solve specific applications:\n",
    "\n",
    "- Avoid penalization when there are more valid recommendations than the ones requested\n",
    "\n",
    "\\begin{equation*}\n",
    "AP@k​​=​​{\\frac{1}{min(m,N)}}​\\sum_{k=1}^N​P(k)​\\cdot​rel(k)\n",
    "\\end{equation*}\n",
    "\n",
    "* Define AP = 0 if m = 0 (it will move the final MAP down).\n",
    "\n",
    "* Expressed in terms of precision and recall\n",
    "\\begin{equation*}\n",
    "AP@k​​​=​​​\\sum_{k=1}^N​(precision​at​k)​\\cdot​(change​in​recall​at​k)​​​=​​​\\sum_{k=1}^N​P(k)​\\Delta r(k)\n",
    "\\end{equation*}\n",
    "In this formulation we don't need the 1/m as the change in recall is exactly 1/m\n",
    "\n",
    "\n",
    "#### Notes\n",
    "- MAP is popular in information retrieval (google search) and recommendations (products).\n",
    "- Evaluates if the results are relevant and how they are sorted.\n",
    "- Implies a ranking factor and is very relevant if we want the top recommendations first.\n",
    "\n",
    "\n",
    "### **Normalized Discounted Cumulative Gain**\n",
    "NDCG computes a *relevance* score (gain) for each item. If we don't have feedback for an item, the gain is zero. Adding all the gains is the cumulative part but as we want the most relevant items first, we divide each gain by a growing number (typically a logarithm of the item position) which is the *discounting*.\n",
    "\n",
    "\\begin{equation*}\n",
    "DCG@k​​​=​​​\\sum_{i=1}^k​{\\frac{rel_i}{log_2(i​+​1)}}\n",
    "\\end{equation*}\n",
    "\n",
    "Finally because DCGs are not directly comparable across users, we normalize them. The worst possible DCG is zero. For the best, we arrange the items in the test set in the ideal order, take the first *k* items and compute the DCG. Then we divide the raw DCG by this ideal DCT and that's the NDCG@k (a metric between 0 and 1).\n",
    "\n",
    "\\begin{equation*}\n",
    "NDCG@k​​​=​​​{\\frac{DCG@k}{IDCG@k}}\n",
    "\\end{equation*}\n",
    "\n",
    "An important note is that the test set consists of all items outside the train set including those not ranked by the user. Some people restrict the test set to the user’s held-out ratings, so the recommender’s task is reduced to ordering those relatively few items but this is not a realistic scenario.\n",
    "\n",
    "#### DCG example:\n",
    "- Given 6 scores from 0 to 3: [3,2,3,0,1,2]\n",
    "- CG<sub>6</sub> = 3 + 2 + 3 + 0 + 1 + 2 = 11  Note that changing the order of the document do not change CG.\n",
    "- DCG adds the relevance of how they are sorted:\n",
    "\n",
    "| i | rel i | log2(i+1) | (rel i) / (log2 (i+1) |\n",
    "|:-----------:|:-----------:|:-----------:|:-----------:|\n",
    "| 1 | 3 |   1   |   3   |\n",
    "| 2 | 2 | 1.585 | 1.262 |\n",
    "| 3 | 3 |   2   |  1.5  |\n",
    "| 4 | 0 | 2.322 |   0   |\n",
    "| 5 | 1 | 2.585 | 0.387 |\n",
    "| 6 | 2 | 2.807 | 0.712 |\n",
    "\n",
    "- DCG<sub>6</sub> = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861\n",
    "\n",
    "### MAP vs NDCG\n",
    "- MAP is a metric for binary feedback while NDCG can be used in any scenario with a relevance score (binary, integer or real).\n",
    "\n",
    "### Weak and Strong generalization\n",
    "Users and items can be divided in two groups: those in the training set and those not in it.<br>\n",
    "The validation of scores for the elements within the training set is the **weak generalization**, and for the others, outside the training set, is the **strong generalization**.\n",
    "\n",
    "Strong generalization account for real life problems like **cold start**, that is users/items from which we do not have previous information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.7.0\n",
      "\n",
      "numpy 1.16.4\n",
      "pandas 0.25.0\n",
      "skmultilearn not installed\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 18.7.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : b36090b079be394cd81bc649c2ed1662202c1ad8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,pandas,skmultilearn -g\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import yaml\n",
    "import dateutil\n",
    "import watermark\n",
    "from tqdm import tqdm\n",
    "from math import floor\n",
    "from pprint import pprint as pp\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "from pandas.plotting import register_matplotlib_converters    # for pandas_profiling\n",
    "\n",
    "from fastai.collab import * \n",
    "\n",
    "register_matplotlib_converters()                              # for pandas_profiling\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.recommenders import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ratings and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('./data/raw/movies_metadata.csv', low_memory=False)\n",
    "ratings_sm = pd.read_csv(\"./data/raw/ratings_small.zip\", low_memory=False)\n",
    "credits = pd.read_csv('./data/raw/credits.csv')\n",
    "keywords = pd.read_csv('./data/raw/keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply cleaning and transformations as in previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with bad IDs.\n",
    "bad_ids =  [idx for idx, k in zip(metadata.index, metadata['id'].values) if not k.isdigit()]\n",
    "metadata = metadata.drop(bad_ids)\n",
    "\n",
    "metadata['id'] = metadata['id'].astype('int')\n",
    "valid_ids = ratings_sm.movieId.unique()\n",
    "metadata = metadata[metadata['id'].isin(valid_ids)]\n",
    "\n",
    "metadata.release_date.fillna('1980-01-01', inplace=True)\n",
    "metadata['release_date'] = metadata.release_date.apply(dateutil.parser.parse)\n",
    "metadata['overview'] = metadata['overview'].fillna('')\n",
    "\n",
    "# Convert IDs to int. Required for merging\n",
    "keywords['id'] = keywords['id'].astype('int')\n",
    "credits['id'] = credits['id'].astype('int')\n",
    "\n",
    "# Merge keywords and credits into your main metadata dataframe\n",
    "metadata = metadata.merge(credits, on='id')\n",
    "metadata = metadata.merge(keywords, on='id')\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(x):\n",
    "    \"\"\"Get the director's name from the crew feature. If director is not listed, return NaN\"\"\"\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "metadata['director'] = metadata['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(get_topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply clean_data function to your features.\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination(x):\n",
    "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + \\\n",
    "           ' ' + x['director'] + ' ' + ' '.join(x['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['combination'] = metadata.apply(create_combination, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr, X_te = train_test_split(ratings_sm, test_size=0.25, random_state=42)\n",
    "# metadata_tr = metadata[metadata.id.isin(X_tr.movieId.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = pd.read_csv('./data/processed/X_tr.zip')\n",
    "X_te = pd.read_csv('./data/processed/X_te.zip')\n",
    "metadata = pd.read_csv('./data/processed/metadata.zip')\n",
    "metadata_tr = pd.read_csv('./data/processed/metadata_tr.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr.to_csv('./data/processed/X_tr.zip', index=False, compression='zip')\n",
    "# X_te.to_csv('./data/processed/X_te.zip', index=False, compression='zip')\n",
    "# metadata.to_csv('./data/processed/metadata.zip', index=False, compression='zip')\n",
    "# metadata_tr.to_csv('./data/processed/metadata_tr.zip', index=False, compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular = MostPopular()\n",
    "most_popular.fit(metadata_tr, 'id', vote_count='vote_count', vote_mean='vote_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>w_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>278</td>\n",
       "      <td>8.223751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>238</td>\n",
       "      <td>8.134695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>155</td>\n",
       "      <td>8.121905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>550</td>\n",
       "      <td>8.079547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>680</td>\n",
       "      <td>8.057069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>13</td>\n",
       "      <td>7.956487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1891</td>\n",
       "      <td>7.883946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>424</td>\n",
       "      <td>7.875086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>122</td>\n",
       "      <td>7.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>129</td>\n",
       "      <td>7.836726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   w_score\n",
       "51     278  8.223751\n",
       "127    238  8.134695\n",
       "1803   155  8.121905\n",
       "533    550  8.079547\n",
       "47     680  8.057069\n",
       "59      13  7.956487\n",
       "193   1891  7.883946\n",
       "91     424  7.875086\n",
       "1089   122  7.871403\n",
       "889    129  7.836726"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_popular.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Recommender Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rec = ContentRecommender()\n",
    "content_rec.fit(metadata_tr, 'id', 'overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_recommendations = content_rec.predict(X_tr, 'userId', 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the classifiers that we want to evaluate into the list of models...\n",
    "models = [\n",
    "    binrel_clf,\n",
    "    multinomial_csf,\n",
    "    random_forest_csf,\n",
    "    extra_tree_csf,\n",
    "    ensamble_trees_csf,\n",
    "    multiclass_svm_csf,\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))      # Empty DataFrame to allocate scores\n",
    "entries = []\n",
    "\n",
    "# Score all models\n",
    "for model in models:\n",
    "    # Extract the model name from the pipeline classifier\n",
    "    model_name = [(step[1].classifier.__class__.__name__ if hasattr(step[1], 'classifier') \n",
    "                   else step[1].__class__.__name__ ) for step in model.steps if step[0] == 'clf'][0]\n",
    "    accuracies = cross_val_score(model, X_train, df_bin_labels, scoring='accuracy', cv=CV)\n",
    "    \n",
    "    for idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, idx, accuracy))\n",
    "        \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_index', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
