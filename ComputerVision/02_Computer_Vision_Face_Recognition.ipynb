{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.3\n",
      "pandas 0.25.1\n",
      "sklearn 0.21.3\n",
      "cv2 4.1.1\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 19.0.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : bb97e071ca243a5b74f45496aa291524e912720d\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,pandas,sklearn,cv2 -g\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import watermark\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition\n",
    "\n",
    "In object recognition there are different algorithms and techniques and within the main ones we find:\n",
    "- ImageAI (library)\n",
    "- OpenCV (library)\n",
    "\n",
    "- Single Shot Detectors\n",
    "- YOLO (You only look once)\n",
    "- Region-based Convolutional Neural Networks\n",
    "\n",
    "In the algorithms there are two main categories:\n",
    "- Two Step Detectors: typically slower but more accurate.\n",
    "- Single Shot Detectors: faster but lossing accuracy compared to the two step detectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main topics:\n",
    "1. **Deep metric learning**\n",
    "2. Facial embeddings\n",
    "3. Application to still images as well as video streams\n",
    "\n",
    "We will use python, OpenCV and Deep Learning to do an implementation of Face Recognition. Deep learning-based facial embeddings are both highly accurate and capable of being executed in real-time.\n",
    "\n",
    "\n",
    "### How it works?\n",
    "\n",
    "The big difference is **deep metric learning**. If you have used deep learning you've already seen that typically the process to train a neural network implies:\n",
    "1. Accepting a single input image.\n",
    "2. Generating an output classification or label for that image.\n",
    "\n",
    "But in deep metric learning we instead generate a real-value **feature vector**. For example in a dlib facial recognition network, the output feature vector is 128d (a list with 128 real numeric values) and they quantify the face. And the way to train the neural network is done in **triplets**.\n",
    "\n",
    "In this triplet of images two images are from the same person and the third from a different person (random). The idea behind is that the weights in our model will be adjusted so that the 128d embeddings of the 2 similar images will be closer to each other than the random person embedding.\n",
    "\n",
    "Our network architecture for face recognition is based on **ResNet-34** from the Deep Residual Learning for Image Recognition paper by [He and Zhang](https://arxiv.org/abs/1512.03385), but with fewer layers and the number of filters reduced by half.\n",
    "\n",
    "The network itself was trained by Davis King on a dataset of ~3 million images. On the [Labeled Faces in the Wild (LFW)](http://vis-www.cs.umass.edu/lfw/) dataset the network compares to other state-of-the-art methods, reaching 99.38% accuracy.\n",
    "\n",
    "Both Davis King (the creator of dlib) and Adam Geitgey (the author of the face_recognition module we’ll be using shortly) have written detailed articles on how deep learning-based facial recognition works:\n",
    "\n",
    "- [High Quality Face Recognition with Deep Metric Learning](http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html) (Davis)\n",
    "- [Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78) (Adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extra packages that we require include: dlib, face_recognition and imutils\n",
    "# !{sys.executable} -m pip install --quiet dlib\n",
    "# !{sys.executable} -m pip install --quiet face_recognition\n",
    "\n",
    "# Do not run:\n",
    "# > !pip install dlib\n",
    "# as it may not install in the current active kernel\n",
    "\n",
    "# If using conda:\n",
    "# > !conda install --yes --prefix {sys.prefix} dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding faces with OpenCV and Deep Learning\n",
    "\n",
    "- Create the 128-d embeddings for each face in the dataset: in our case we are using *dlib* which contains our implementation of “deep metric learning” used to construct our face embeddings.\n",
    "- Use these embeddings to recognize the faces of the characters in both images and video streams.\n",
    "- *face_recognition* library wraps dlib and make it easier to operate.\n",
    "\n",
    "Using the pre-trained model, we use it to construct the 128-d embeddings for the faces in our dataset.\n",
    "\n",
    "For the classification we can use a k-NN model (slower but more accurate) or hog model (faster but less accurate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CV libraries\n",
    "import cv2\n",
    "from imutils import paths\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_METHOD = 'cnn'       # cnn or hog\n",
    "IMAGES_FOLDER = \"./data/faces/\"\n",
    "imagePaths = list(paths.list_images(IMAGES_FOLDER))\n",
    "ENCODINGS_OUTPUT = './data/face_encodings.pickle'\n",
    "OUTPUT_FILE = \"./data/usb_output\"\n",
    "DISPLAY = 1\n",
    "\n",
    "# initialize the list of known encodings and known names\n",
    "knownEncodings = []\n",
    "knownNames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing image 001/218...\n",
      "[INFO] processing image 026/218...\n",
      "[INFO] processing image 051/218...\n",
      "[INFO] processing image 076/218...\n",
      "[INFO] processing image 101/218...\n",
      "[INFO] processing image 126/218...\n",
      "[INFO] processing image 151/218...\n",
      "[INFO] processing image 176/218...\n",
      "[INFO] processing image 201/218...\n",
      "[INFO] processing finished!\n"
     ]
    }
   ],
   "source": [
    "# We have 218 faces to loop through and incorporate the names and encodings\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    if i % 25 == 0:\n",
    "        print(f\"[INFO] processing image {str(i + 1).zfill(3)}/{len(imagePaths)}...\")\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "    # load the input image and convert it from RGB (OpenCV ordering) to dlib ordering (RGB)\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # detect the (x, y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input image\n",
    "    boxes = face_recognition.face_locations(rgb, model=DETECTION_METHOD)\n",
    "\n",
    "    # compute the facial embedding for the face\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "    # loop over the encodings\n",
    "    for encoding in encodings:\n",
    "        # add each encoding + name to our set of known names and\n",
    "        # encodings\n",
    "        knownEncodings.append(encoding)\n",
    "        knownNames.append(name)\n",
    "print(f\"[INFO] processing finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing encodings...\n"
     ]
    }
   ],
   "source": [
    "# Now we save the results to the hd\n",
    "print(\"[INFO] serializing encodings...\")\n",
    "data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "with open(ENCODINGS_OUTPUT, \"wb\") as encodings_file:\n",
    "    encodings_file.write(pickle.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the images encoded in our encodings file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing faces on images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n"
     ]
    }
   ],
   "source": [
    "# Load the previously calculated faces and embeddings\n",
    "print(\"[INFO] loading encodings...\")\n",
    "with open(ENCODINGS_OUTPUT, \"rb\") as encodings_file:\n",
    "    data = pickle.loads(encodings_file.read())\n",
    "\n",
    "# load the input image and convert it from BGR to RGB\n",
    "test_image = './data/faces_test/example_01.png'\n",
    "image = cv2.imread(test_image)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] recognizing faces...\n"
     ]
    }
   ],
   "source": [
    "# detect the (x, y)-coordinates of the bounding boxes corresponding to each\n",
    "# face in the input image, then compute the facial embeddings for each face\n",
    "print(\"[INFO] recognizing faces...\")\n",
    "boxes = face_recognition.face_locations(rgb, model=DETECTION_METHOD)\n",
    "encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "# initialize the list of names for each face detected\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the facial embeddings\n",
    "for encoding in encodings:\n",
    "    # attempt to match each face in the input image to our known encodings\n",
    "    matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "    name = \"Unknown\"\n",
    "    \n",
    "    # This function returns a list of True / False  values, one for each image in our dataset. \n",
    "    # therefore the returned list will have 218 boolean values.\n",
    "    # Internally, the compare_faces function is computing the Euclidean distance between \n",
    "    # the candidate embedding and all faces in our dataset and assesing it with the defined tolerance\n",
    "\n",
    "    # check to see if we have found a match\n",
    "    if True in matches:\n",
    "        # find the indexes of all matched faces then initialize a dictionary\n",
    "        #  to count the total number of times each face was matched\n",
    "        matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "        counts = {}\n",
    "\n",
    "        # loop over the matched indexes and maintain a count for each recognized face face\n",
    "        for i in matchedIdxs:\n",
    "            name = data[\"names\"][i]\n",
    "            counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "        # determine the recognized face with the largest number of votes (if there\n",
    "        # is an unlikely tie, python will select first entry in the dictionary)\n",
    "        name = max(counts, key=counts.get)\n",
    "\n",
    "    # update the list of names\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the recognized faces\n",
    "progress = tqdm_notebook if in_ipython() else tqdm\n",
    "for ((top, right, bottom, left), name) in progress(zip(boxes, names)):\n",
    "    # draw the predicted face name on the image\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "    y = top - 15 if top - 15 > 15 else top + 15\n",
    "    cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "# load image using cv2....and do processing.\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "# as opencv loads in BGR format by default, we want to show it in RGB.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognizing faces on video\n",
    "\n",
    "Now we'll make sure we use hog or even OpenCV Haar cascades to avoid performance issues but the rest  of the script is similar to the one we have already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "# Initialize the video stream and pointer to output video file, then allow the camera sensor to warm up\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()           # src=0 for our first camera\n",
    "writer = None\n",
    "time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6e1a85ad9db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# corresponding to each face in the input frame, then compute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# the facial embeddings for each face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDETECTION_METHOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \"\"\"\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read()\n",
    "    \n",
    "    # convert the input frame from BGR to RGB then resize it to have\n",
    "    # a width of 750px (to speedup processing)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb = imutils.resize(frame, width=750)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "\n",
    "    # detect the (x, y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input frame, then compute\n",
    "    # the facial embeddings for each face\n",
    "    boxes = face_recognition.face_locations(rgb, model=DETECTION_METHOD)\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "\n",
    "    # loop over the facial embeddings\n",
    "    for encoding in encodings:\n",
    "        # attempt to match each face in the input image to our known encodings\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"], encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # check to see if we have found a match\n",
    "        if True in matches:\n",
    "            # find the indexes of all matched faces then initialize a\n",
    "            # dictionary to count the total number of times each face was matched\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "\n",
    "            # loop over the matched indexes and maintain a count for\n",
    "            # each recognized face face\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "\n",
    "            # determine the recognized face with the largest number of votes (in the event\n",
    "            # of an unlikely tie Python, will select first entry in the dictionary)\n",
    "            name = max(counts, key=counts.get)\n",
    "\n",
    "        # update the list of names\n",
    "        names.append(name)\n",
    "\n",
    "    # loop over the recognized faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        # rescale the face coordinates\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "\n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    # if the video writer is None *AND* we are supposed to write\n",
    "    # the output video to disk initialize the writer\n",
    "    if writer is None and OUTPUT_FILE is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, 20, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # if the writer is not None, write the frame with recognized faces t odisk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "    # check to see if we are supposed to display the output frame to the screen\n",
    "    if DISPLAY > 0:\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        # as opencv loads in BGR format by default, we want to show it in RGB.\n",
    "        plt.show()\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
