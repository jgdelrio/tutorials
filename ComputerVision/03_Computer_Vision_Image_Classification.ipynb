{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to train a model for image recognition is finding images that belong to the desired class (or classes).\n",
    "\n",
    "\n",
    "### ImageNet\n",
    "\n",
    "This dataset is publicly available which the goal of promoting the development of computer vision methods. ImageNet currently has 14,197,122 images with 21841 synsets indexed (see [Imagenet](http://www.image-net.org/)).\n",
    "\n",
    "ImageNet aims to provide on average 1k images to illustrate each one of their 100k synsets, the majority of the synsets are nouns (80.000+). The **synsets** (synonym sets) come from WordNet which is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.\n",
    "\n",
    "Check more [information about ImageNet](http://www.image-net.org/about-overview).\n",
    "\n",
    "Check more [information about WordNet](https://wordnet.princeton.edu/).\n",
    "These useful classified images can be obtained using Python.\n",
    "\n",
    "We will use for this task the **ImageNet** dataset. This is the dataset of the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) which is a popular challenge that has brought many important innovations. \n",
    "\n",
    "Typical challenges performed in this dataset include:\n",
    "\n",
    "- Image classification: Predict the classes of objects present.\n",
    "- Object localization: Image classification (and draw a bounding box around one example of each object present).\n",
    "- Object detection: Image classification (and draw a bounding box around each object present).\n",
    "- Labeling videos.\n",
    "\n",
    "During the first five years the pace of improvements have been dramatic, with great success using CNNs and the papers published have become a must read.\n",
    "\n",
    "<img src=\"./fig/ILSVRC_improvements.png\" alt=\"RGB explination\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Some of the techniques applied to improve the accuracy of the predictions include:\n",
    "\n",
    "#### Basics:\n",
    "* CNNs: Convolutional Neural Networks. It has been the technique with greater success in the last years\n",
    "* ReLUS: Neurons with nonlinearity as Rectified Linear Units as they train faster than for example tanh units and they do not require input normalization to prevent saturation, although local normalization still helps generalization. Response normalization helps to reduce the final error.\n",
    "\n",
    "#### Reducing Overfitting:\n",
    "* Data Augmentation: \n",
    "    - Applying transformations (translation, rotations, zooming...) having the dataset multiplied by factors of 2048.\n",
    "    - PCA performed on the RGB values, altering the intensities of the channels and adding to each image multiples of the principal componenets found with magnitues proportional to the corresponding eigenvalues itmes a random variables (from a Gaussian with mean 0 and st dev 01)\n",
    "* Dropout: Combining predictions of many models is very effective to reduce errors but it's way too expensive for big neural networks that take days to train. Dropouts sets to zero the output of each hidden node with a probability of 0.5 and this way the drop out neurons do not contribute to the forward pass nor they participate on the back-propagation. So every time the NN presents a different architecture but all of them share weights. This technique reduces co-adaptations of neurons as they cannot rely on the presence of other particular neurons and they are forced to learn more robust features.\n",
    "\n",
    "#### Some Details extracted from the papers:\n",
    "- Stochastic gradient descent for training with batches of 128, momentum of 0.9 and weight decay of 0.0005 which was not merely a regularizer but it reduces as well the training error.\n",
    "- The initialization was done with a Gaussian distribution with st. dev. 0.01\n",
    "- The learning rate at 0.01 and reduced three times prior to termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "CPython 3.7.3\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.2\n",
      "pandas 0.25.2\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.0.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 5.0.0-31-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "Git hash   : cb13d8abe00615853de212fe6df32ebd0d3729bb\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,pandas,sklearn,tensorflow -g\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import watermark\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Introduction**\n",
    "<br>\n",
    "\n",
    "2. **Data preparation**\n",
    "    - 2.1 Load data\n",
    "    - 2.2 Check for null and missing values\n",
    "    - 2.3 Normalization\n",
    "    - 2.4 Reshape\n",
    "    - 2.5 Label encoding\n",
    "    - 2.6 Split training and valdiation set\n",
    "\n",
    "\n",
    "3. **CNN**\n",
    "    - 3.1 Define the model\n",
    "    - 3.2 Set the optimizer and annealer\n",
    "    - 3.3 Data augmentation\n",
    "\n",
    "\n",
    "4. **Model Evaluation**\n",
    "    - 4.1 Training and validation curves\n",
    "    - 4.2 Confusion matrix\n",
    "\n",
    "\n",
    "5. **Prediction**\n",
    "    - 5.1 Predict samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This is a 5 layers Sequential CNN build with **Keras API** and trained on MNIST dataset.\n",
    "\n",
    "The model achieves a very high accuracy (99.671%) with this CNN trained in 2h30 on a single CPU (i5 2500k). If you have a GPU at hand you can use tensorflow-gpu with keras. Computation will be much much faster!!!\n",
    "\n",
    "Initially the number of epochs 2 but to achive the highest accurary you can set it to 30 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "### 2.1 Load data\n",
    "\n",
    "The data is separated between the train and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv(\"./data/mnist/train.csv\")\n",
    "df_te = pd.read_csv(\"./data/mnist/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count per category:\n",
      "1    4684\n",
      "7    4401\n",
      "3    4351\n",
      "9    4188\n",
      "2    4177\n",
      "6    4137\n",
      "0    4132\n",
      "4    4072\n",
      "8    4063\n",
      "5    3795\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_tr = df_tr[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_tr = df_tr.drop(labels = [\"label\"], axis=1) \n",
    "\n",
    "# free space\n",
    "del df_tr\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "g = sns.countplot(Y_tr)\n",
    "\n",
    "print(f\"Count per category:\\n{Y_tr.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in the upper range: 11.5\n",
      "Difference in the lower range: -9.6\n"
     ]
    }
   ],
   "source": [
    "count_mean = Y_tr.value_counts().mean()\n",
    "upper_lvl = 100 * (Y_tr.value_counts().max() - count_mean)/count_mean\n",
    "lower_lvl = 100 * (Y_tr.value_counts().min() - count_mean)/count_mean\n",
    "print(f\"Difference in the upper range: {upper_lvl:.1f}\")\n",
    "print(f\"Difference in the lower range: {lower_lvl:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in the counts is around +/- 10% so in this case we can consider that the distrubution is ok.\n",
    "\n",
    "It's important to verify that we have a nice distribution of cases across all the categories to avoid having a model trained mainly in one category. If that happens the model will likely perform poorly in the classes with fewer cases in the training set.\n",
    "\n",
    "Otherwise we may consider certain techniques to compensate/solve the issue:\n",
    "- Stratified sampling\n",
    "- Selected metrics: Precision@Recall=x or FPR@Recall=x  or  Recall@Precision=x\n",
    "- Cost-sensitive learning\n",
    "- SMOTE and ROSE algorithms. SMOTE effectively uses a k-nearest neighbours approach to exclude members of the majority class while in a similar way creating synthetic examples of a minority class. ROSE tries to create estimates of the underlying distributions of the two classes using a smoothed bootstrap approach and sample them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check for null and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "X_tr.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_te.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we don't have missing values in the train nor in the test dataset. Otherwise we may have to drop the entries that don't help to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalization\n",
    "\n",
    "We perform a grayscale normalization to reduce the effect of illumination's differences.\n",
    "\n",
    "This helps as well to achieve a faster convergence ([0..1] data vs. [0..255])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_tr = X_tr / 255.0\n",
    "df_te = df_te / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image from row with 784 values to 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_tr = X_tr.values.reshape(-1, 28, 28, 1)\n",
    "df_te = df_te.values.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test images (28px x 28px) has been stock into pandas as a matrix where we have 1D vectors of 784 values. We reshape all data to 28x28x1 3D matrices.\n",
    "\n",
    "Keras requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so it uses only one channel. For RGB images there are 3 channels so we would have reshaped 784px vectors to 28x28x3 3D matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_tr = to_categorical(Y_tr, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Split training and valdiation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and the validation set for the fitting\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_tr, Y_tr, test_size=0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the already split data (train and test) we retain a small fraction of the train data (10%) to become the **validation set** in which the model is evaluated and the rest (90%) is used to train the model.\n",
    "\n",
    "Since we have 42 000 training images of balanced labels a random split of the train set doesn't cause some labels to be over represented in the validation set. Be carefull with some unbalanced dataset where a simple random split could cause inaccurate evaluation during the validation.\n",
    "\n",
    "To avoid that, you could use stratify=True option in train_test_split function (only for >=0.17 sklearn versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examples of the data\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)                            # subplot def\n",
    "    plt.imshow(X_tr[i][:,:,0], cmap=plt.get_cmap('gray'))      # raw pixel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful as well to understand the distribution of the gradation of the color layer that we have. Humans are limited and we see 100 levels of gradation of a single color (in this case, white/grey/black) and even fewer with the limitations of a computer display, but we may have images with up to 2^16 == 65536 levels in a 16 bit image and the CNN won't have problems seeing those differences as it takes as input floating point data.\n",
    "\n",
    "For human purposes we may apply windowing and get 3x levels by using 3 channel image with different windows in each channel.\n",
    "\n",
    "On the other hand this doesn't mean that we can ignore scaling entirely for the CNN as having well-scaled inputs is really important in getting good results from your NN. That means we want to see a good mix of values throughout the range of our data (e.g. something having approximately a normal or uniform distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "px = X_tr.flatten()\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.hist(px, bins=80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the graph shows, we don't have that at all.\n",
    "\n",
    "We have a highly bimodal distribution, with lots of background pixels at around 0 and the digits at 1. But that's OK, we can normalize this using a simple scaling of the pixel values using a non-linear mapping designed to give us an equal number of pixels in each range. To do that we first split the range of pixel values into groups, such that each group has around the same number of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'freqhist_bins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-5f1bf4bb1d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqhist_bins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'freqhist_bins'"
     ]
    }
   ],
   "source": [
    "bins = px.freqhist_bins(20)\n",
    "print(bins)\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.hist(px, bins=bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case as we only have 256 values it is not going to make difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN\n",
    "### 3.1 Define the model\n",
    "\n",
    "In tensorflow 2.0 Keras is already included as its Sequential API is really easy to use. With the Sequential API you just have just to add one layer at a time starting from the input.\n",
    "\n",
    "First is the **convolutional (Conv2D) layer**, a set of learnable filters. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n",
    "\n",
    "The CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n",
    "\n",
    "The second important layer in CNN is the **pooling (MaxPool2D) layer**. This layer simply acts as a downsampling filter by looking at the 2 neighboring pixels and picking the maximal value. This technique is used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size. The highest the pooling dimension, more the downsampling is important.\n",
    "\n",
    "Combining convolutional and pooling layers CNN are able to combine local features and learn more global features of the image.\n",
    "\n",
    "**Dropout** is a regularization method, where a proportion of nodes in the layer are randomly set to zero with 0.5 probability for each training sample. This forces the network to learn features in a distributed way and the NN have to learn independenly from each other. This technique also improves generalization and reduces the overfitting.\n",
    "\n",
    "**ReLU** is the rectifier, and activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n",
    "\n",
    "The **Flatten layer** is used to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers.\n",
    "\n",
    "And finally two fully-connected **(Dense) layers** which is just an Artificial Neural Networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs  the distribution of probability for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CNN model with architechture:\n",
    "#    In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters =32, kernel_size=(5,5), padding=\"Same\", \n",
    "                 activation=\"relu\", input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"Same\",\n",
    "                 activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"Same\",\n",
    "                 activation =\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"Same\",\n",
    "                 activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set the optimizer and annealer\n",
    "\n",
    "Now we need to set up a **score function** a **loss function** and an **optimisation algorithm**.\n",
    "\n",
    "We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the ***\"categorical_crossentropy\"***.\n",
    "\n",
    "The most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss. RMSprop (with default values) is a very effective optimizer. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate. We could also have used Stochastic Gradient Descent ('sgd') optimizer, but it is slower than RMSprop.\n",
    "\n",
    "The metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the optimizer converge faster and closest to the global minimum of the loss function, we use an **annealing method** of the learning rate (LR).\n",
    "\n",
    "The LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n",
    "\n",
    "Its better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n",
    "\n",
    "To keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n",
    "\n",
    "With the ReduceLROnPlateau function from Keras.callbacks, we choose to reduce the LR by half if the accuracy is not improved after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data augmentation\n",
    "\n",
    "In order to **avoid overfitting** we need to expand artificially our handwritten digit dataset. We can make the existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n",
    "\n",
    "Some operations we can perform while keeping the same labels, include:\n",
    "\n",
    "- Translation\n",
    "- Rotation\n",
    "- Zooming\n",
    "- Random crops\n",
    "- Grayscales\n",
    "- Color jitters\n",
    "- Horizontal flips\n",
    "- Vertical flips\n",
    "\n",
    "This way we can triple or more the number of training examples and create a very robust model. This is an important step as it provides a relevant improvement to the model:\n",
    "\n",
    "- Without data augmentation: accuracy of 98.114%\n",
    "- With data augmentation:  99.67% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without data augmentation\n",
    "trained_model_no_aug = model.fit(X_tr, Y_tr, batch_size=batch_size, epochs=epochs, \n",
    "                                 validation_data=(X_val, Y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,             # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,              # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,   # divide each input by its std\n",
    "    zca_whitening=False,                  # apply ZCA whitening\n",
    "    rotation_range=10,                    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1,                     # Randomly zoom image \n",
    "    width_shift_range=0.1,                # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,               # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,                # randomly flip images\n",
    "    vertical_flip=False)                  # randomly flip images\n",
    "\n",
    "datagen.fit(X_tra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data augmentation:\n",
    "\n",
    "- Randomly rotate some training images by 10 degrees\n",
    "- Randomly Zoom by 10%\n",
    "- Randomly shift images horizontally by 10% of the width\n",
    "- Randomly shift images vertically by 10% of the height\n",
    "- Not vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(X_tr,Y_tr, batch_size=batch_size),\n",
    "                              epochs=epochs, validation_data=(X_val, Y_val),\n",
    "                              verbose=2, steps_per_epoch=X_train.shape[0] // batch_size, \n",
    "                              callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "### 4.1 Training and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is for plotting loss and accuracy curves for training and validation. Below it's the training and validation curves with 30 epochs (2h30)\n",
    "\n",
    "\n",
    "\n",
    "The model reaches almost 99% (98.7+%) accuracy on the validation dataset after 2 epochs. The validation accuracy is greater than the training accuracy almost every time during the training. That means that our model doesn't overfit the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Confusion matrix\n",
    "\n",
    "The confusion matrix is very helpfull to see the model drawbacks.\n",
    "\n",
    "This is the confusion matrix on the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title=\"Confusion matrix\",\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val, axis=1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes=range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the CNN performs very well on all digits with few errors considering the size of the validation set (4200 images).\n",
    "\n",
    "However, it seems that our CNN has some troubles with the 4s and some are misclassified as 9s.\n",
    "\n",
    "Let's investigate for errors.\n",
    "\n",
    "We want to see the most important errors. For that purpose we select the biggest differences between the probabilities of the real label and the predicted labels in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\n",
    "                f\"Predicted label :{pred_errors[error]}\\nTrue label :{obs_errors[error]}\")\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important errors are also the most interesting ones.\n",
    "\n",
    "It is clear that for the selected cases the model is not doing anything strange, in fact the errors are quite human as many people will find difficult to read these digits. We see that the 9 it's very close to 4 as well as the last 9 is very misleading, much closer to a 0 than to a nine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction\n",
    "### 5.1 Predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name=\"ImageId\"), results], axis=1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [ImageNet: A Large-Scale Hierarchical Image Database](https://ieeexplore.ieee.org/document/5206848), 2009.\n",
    "\n",
    "[2] [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks), 2012.\n",
    "\n",
    "[3] [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901), 2013.\n",
    "\n",
    "[4] [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842), 2014.\n",
    "\n",
    "[5] [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556), 2015.\n",
    "\n",
    "[6] [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), 2015.\n",
    "\n",
    "[7] [ImageNet Large Scale Visual Recognition Challenge](https://link.springer.com/article/10.1007/s11263-015-0816-y), 2015.\n",
    "\n",
    "[8] [Image Classification transfer learning with Inception v3](https://codelabs.developers.google.com/codelabs/cpb102-txf-learning/index.html#0)\n",
    "\n",
    "[9] [Advanced Guide to Inception v3 on Cloud TPU](https://cloud.google.com/tpu/docs/inception-v3-advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
