{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision: Using a Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classification is based on the **Plant Seedlings Dataset**, which contains images of approximately 960 unique plants belonging to 12 species at several growth stages, with a resolution of about 10 pixels per mm of annotated RGB images.\n",
    "\n",
    "The dataset includes the following species:\n",
    "\n",
    "\n",
    "|English     |Latin               |EPPO|\n",
    "|:-----------|:-------------------|:---|\n",
    "|Maize       |Zea mays L.         |ZEAMX|\n",
    "|Common wheat|Triticum aestivum L.|TRZAX|\n",
    "|Sugar beet|Beta vulgaris var. altissima|BEAVA|\n",
    "|Scentless Mayweed|Matricaria perforata Mérat|MATIN|\n",
    "|Common Chickweed|Stellaria media|STEME|\n",
    "|Shepherd’s Purse|Capsella bursa-pastoris|CAPBP|\n",
    "|Cleavers|Galium aparine L.|GALAP|\n",
    "|Charlock|Sinapis arvensis L.|SINAR|\n",
    "|Fat Hen|Chenopodium album L.|CHEAL|\n",
    "|Small-flowered Cranesbill|Geranium pusillum|GERSS|\n",
    "|Black-grass|Alopecurus myosuroides|ALOMY|\n",
    "|Loose Silky-bent|Apera spica-venti|APESV|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.3\n",
      "pandas 0.25.1\n",
      "sklearn 0.21.3\n",
      "tensorflow 2.0.0\n",
      "\n",
      "compiler   : Clang 4.0.1 (tags/RELEASE_401/final)\n",
      "system     : Darwin\n",
      "release    : 19.0.0\n",
      "machine    : x86_64\n",
      "processor  : i386\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : c6b9079d9be47f8dbb798dbc93d7f425d1c0a382\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,pandas,sklearn,tensorflow -g\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "import pickle\n",
    "import pathlib\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "import watermark\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries specific of Deep learning and images\n",
    "import imageio\n",
    "from skimage.transform import resize as imresize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "import tensorflow as tf2\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Input, InputLayer, Activation, Maximum, ZeroPadding2D, concatenate, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.pardir)\n",
    "from src.log_manager import Logger\n",
    "from src.text_func import list_abbreviations\n",
    "from src.cv_tools import equalizer_augmentation\n",
    "# from src.cv_models import inception_resnet_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "\n",
    "### 1.1 Using previous notebook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logger\n",
    "logger = Logger(\"CNN\", \"cnn_logs.txt\", file_path=os.getcwd(), level=\"INFO\")\n",
    "\n",
    "# Definitions\n",
    "PLANT_CLASSES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', \n",
    "                 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', \n",
    "                 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n",
    "CLASSES_DICT_NAMES = {name: k for k, name in zip(range(len(PLANT_CLASSES)), PLANT_CLASSES)}\n",
    "CLASSES_DICT_NUM = {k: name for k, name in zip(range(len(PLANT_CLASSES)), PLANT_CLASSES)}\n",
    "NUM_CATEGORIES = len(PLANT_CLASSES)\n",
    "ABBREVIATIONS = list_abbreviations(PLANT_CLASSES)\n",
    "RESHAPE_SIZE = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 49 images for label 3\n",
      "Generating 155 images for label 8\n",
      "Generating 186 images for label 10\n",
      "Generating 224 images for label 5\n",
      "Generating 299 images for label 11\n",
      "Generating 310 images for label 1\n",
      "Generating 427 images for label 2\n",
      "Generating 453 images for label 0\n",
      "Generating 488 images for label 9\n",
      "Generating 505 images for label 7\n",
      "Generating 509 images for label 4\n"
     ]
    }
   ],
   "source": [
    "IMAGES = \"./data/plants/all_images.gz\"\n",
    "CLASSES = \"./data/plants/all_classes.gz\"\n",
    "\n",
    "df = pd.read_csv(IMAGES)\n",
    "df_classes = pd.read_csv(CLASSES)\n",
    "df_classes['class'] = df_classes['class'].swiftly.apply(lambda x: CLASSES_DICT_NAMES[x.replace(\"’\", \"\")])\n",
    "\n",
    "# Apply the shape required\n",
    "df = df.values.reshape(-1, *RESHAPE_SIZE)\n",
    "# Balance the dataset so each class has similar number of samples\n",
    "images_generated, labels_generated = equalizer_augmentation(df, df_classes, 'class')\n",
    "\n",
    "# Encode labels to one hot vectors\n",
    "n_unique = np.unique(labels_generated).size\n",
    "labels_encoded = to_categorical(labels_generated, num_classes=n_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = train_test_split(images_generated, labels_encoded, \n",
    "                                          test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(\n",
    "    featurewise_center=False,             # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,              # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,   # divide each input by its std\n",
    "    zca_whitening=False,                  # apply ZCA whitening\n",
    "    rotation_range=10,                    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1,                     # Randomly zoom image \n",
    "    width_shift_range=0.1,                # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,               # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,                # randomly flip images\n",
    "    vertical_flip=False)                  # randomly flip images\n",
    "\n",
    "train_generator.fit(X_tr)\n",
    "\n",
    "val_generator = ImageDataGenerator(\n",
    "    featurewise_center=False,             # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,              # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,   # divide each input by its std\n",
    "    zca_whitening=False,                  # apply ZCA whitening\n",
    "    rotation_range=10,                    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1,                     # Randomly zoom image \n",
    "    width_shift_range=0.1,                # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,               # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,                # randomly flip images\n",
    "    vertical_flip=False)                  # randomly flip images\n",
    "\n",
    "val_generator.fit(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Trained Model\n",
    "\n",
    "There are several ways of creating a CNN model but in this case we will use Keras deep learning library and we will also use the available pretrained models in Keras, trained over ImageNet dataset that we will fine tune for our specific task.\n",
    "\n",
    "It is quite inefficient to train a Convolution Neural Network from scratch except for learning purposes or some edge cases. So we take the weights of a pre trained CNN model on ImageNet with 1000 classes and fine tuning it by keeping some layers frozen and unfreezing some of them for our fine tuning training.\n",
    "\n",
    "- The top layers learn simple basic features. We need not to train those layers as they can be directly applied to our task.\n",
    "- The output differs (from 1000 classes to 12) so we add a final output layer with 12 classes.\n",
    "- Then we will unfreeze some of the last layers and train them.\n",
    "- We must check whether our dataset is similar to ImageNet and how big is our dataset. These 2 parameters will decide how we shoould perform the fine tuning. To know more in detail, read more from [Andrej Karpathy](https://medium.com/u/ac9d9a35533e?source=post_page-----9c1188157a86----------------------)\n",
    "\n",
    "In our case, the dataset is small but a bit similar to ImageNet.\n",
    "\n",
    "We will use Keras for initial benchmarks as Keras provides a number of pretrained models and we will use the ResNet50 and InceptionResNetV2 for our task. It is important to benchmark the dataset with one simple model and one very high end model to understand if we are overfitting/underfitting the dataset on the given model.\n",
    "\n",
    "We can check available models [here](https://keras.io/applications/)\n",
    "\n",
    "### 2.1 MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 12\n",
    "METRIC = \"val_accuracy\"\n",
    "\n",
    "pretrained_MobileNetV2 = applications.MobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show first layers of the architecture\n",
    "summary = []\n",
    "pretrained_MobileNetV2.summary(print_fn=lambda x: summary.append(x))\n",
    "print(\"\\n\".join(summary[:12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Dense)                  (None, 1000)         1281000     global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 3,538,984\n",
      "Trainable params: 3,504,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show last layers of the architecture\n",
    "summary = []\n",
    "pretrained_MobileNetV2.summary(print_fn=lambda x: summary.append(x))\n",
    "print(\"\\n\".join(summary[-14:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           out_relu[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove last layer\n",
    "pretrained_MobileNetV2._layers.pop()\n",
    "# Fix layers that we don't want to re-train\n",
    "for layer in pretrained_MobileNetV2.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "summary = []\n",
    "pretrained_MobileNetV2.summary(print_fn=lambda x: summary.append(x))\n",
    "print(\"\\n\".join(summary[-12:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ts = pretrained_MobileNetV2.input\n",
    "output_ts = pretrained_MobileNetV2.layers[-1].output\n",
    "\n",
    "x = output_ts\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "x = Dropout(0.5, noise_shape=None, seed=None)(x)\n",
    "x = Dense(1280, activation='relu')(x)\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "x = Dropout(0.5, noise_shape=None, seed=None)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(OUTPUT_CLASSES, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_MobileNetV2 = Model(inputs=input_ts, outputs=predictions)\n",
    "optimizer = Adam(lr=0.001)\n",
    "custom_MobileNetV2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "CUSTOM_MOBILENET_WEIGHTS = \"./models/custom_MobileNetV2.hdf5\"\n",
    "lr_reducer = ReduceLROnPlateau(monitor=METRIC, factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=1e-5)\n",
    "model_checkpoint= ModelCheckpoint(CUSTOM_MOBILENET_WEIGHTS, \n",
    "                                  monitor=METRIC, \n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=True, \n",
    "                                  verbose=1)\n",
    "callbacks = [lr_reducer, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 1.0534 - accuracy: 0.6844 WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.08686, saving model to ./models/custom_MobileNetV2.hdf5\n",
      "127/127 [==============================] - 977s 8s/step - loss: 1.0479 - accuracy: 0.6857 - val_loss: 8.3727 - val_accuracy: 0.0869\n",
      "Epoch 2/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.4449 - accuracy: 0.8660 WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.08686 to 0.16258, saving model to ./models/custom_MobileNetV2.hdf5\n",
      "127/127 [==============================] - 963s 8s/step - loss: 0.4440 - accuracy: 0.8665 - val_loss: 8.0314 - val_accuracy: 0.1626\n",
      "Epoch 3/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.4092 - accuracy: 0.8813 WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.16258\n",
      "127/127 [==============================] - 1017s 8s/step - loss: 0.4109 - accuracy: 0.8813 - val_loss: 10.8400 - val_accuracy: 0.1125\n",
      "Epoch 4/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.3169 - accuracy: 0.9124 WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.16258\n",
      "127/127 [==============================] - 1051s 8s/step - loss: 0.3161 - accuracy: 0.9123 - val_loss: 11.6459 - val_accuracy: 0.0980\n",
      "Epoch 5/5\n",
      "126/127 [============================>.] - ETA: 8s - loss: 0.3133 - accuracy: 0.9081 WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.16258\n",
      "127/127 [==============================] - 1177s 9s/step - loss: 0.3153 - accuracy: 0.9079 - val_loss: 11.6466 - val_accuracy: 0.0947\n",
      "CPU times: user 5h 57min 8s, sys: 5h 51min 4s, total: 11h 48min 12s\n",
      "Wall time: 1h 27min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x148db1c18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "custom_MobileNetV2.fit_generator(train_generator.flow(X_tr, Y_tr, batch_size=batch_size), \n",
    "                                 epochs=epochs, \n",
    "                                 workers=4, \n",
    "                                 callbacks=callbacks,\n",
    "                                 validation_data=val_generator.flow(X_te, Y_te, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With the pre-trained model, we get:\n",
    "- 87% accuracy on the 1<sup>st</sup> epoch vs ground up build models where it took 10 epochs to reach around 50%.\n",
    "- 91% accuracy on the 5<sup>th</sup> epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_MobileNetV2.save_weights(CUSTOM_MOBILENET_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout_4 (Dropout)             (None, 1280)         0           batch_normalization_1425[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1280)         1639680     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1426 (Batch (None, 1280)         5120        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1280)         0           batch_normalization_1426[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          655872      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           6156        dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,569,932\n",
      "Trainable params: 4,530,700\n",
      "Non-trainable params: 39,232\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create custom model:\n",
    "\n",
    "# 1) Copy all layers except the output layer\n",
    "custom_InceptionResNetV2 = applications.InceptionResNetV2()\n",
    "custom_InceptionResNetV2._layers.pop()\n",
    "\n",
    "# 2) Make all layers non-trainable\n",
    "for layer in custom_InceptionResNetV2.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# # 3) Set new output layers\n",
    "input_ts = pretrained_MobileNetV2.input\n",
    "output_ts = pretrained_MobileNetV2.layers[-1].output\n",
    "#x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(output_ts)\n",
    "x = Dropout(0.5, noise_shape=None, seed=None)(x)\n",
    "x = Dense(1280, activation='relu')(x)\n",
    "x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
    "x = Dropout(0.5, noise_shape=None, seed=None)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "predictions = Dense(OUTPUT_CLASSES, activation='softmax')(x)\n",
    "\n",
    "custom_InceptionResNetV2 = Model(inputs=input_ts, outputs=predictions)\n",
    "\n",
    "summary = []\n",
    "custom_InceptionResNetV2.summary(print_fn=lambda x: summary.append(x))\n",
    "print(\"\\n\".join(summary[-16:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001)\n",
    "custom_InceptionResNetV2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "CUSTOM_INCEPTION_WEIGHTS = \"./models/custom_InceptionResNetV2.hdf5\"\n",
    "lr_reducer = ReduceLROnPlateau(monitor=METRIC, factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=1e-5)\n",
    "model_checkpoint= ModelCheckpoint(CUSTOM_INCEPTION_WEIGHTS, \n",
    "                                  monitor=METRIC, \n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=True, \n",
    "                                  verbose=1)\n",
    "callbacks = [lr_reducer, model_checkpoint]\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.5415 - accuracy: 0.8568 \n",
      "Epoch 00001: val_accuracy improved from -inf to 0.07238, saving model to ./models/custom_InceptionResNetV2.hdf5\n",
      "127/127 [==============================] - 980s 8s/step - loss: 0.5409 - accuracy: 0.8568 - val_loss: 11.1776 - val_accuracy: 0.0724\n",
      "Epoch 2/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.4633 - accuracy: 0.8743 \n",
      "Epoch 00002: val_accuracy improved from 0.07238 to 0.09243, saving model to ./models/custom_InceptionResNetV2.hdf5\n",
      "127/127 [==============================] - 976s 8s/step - loss: 0.4620 - accuracy: 0.8742 - val_loss: 14.5103 - val_accuracy: 0.0924\n",
      "Epoch 3/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.2594 - accuracy: 0.9205 \n",
      "Epoch 00003: val_accuracy improved from 0.09243 to 0.14031, saving model to ./models/custom_InceptionResNetV2.hdf5\n",
      "127/127 [==============================] - 956s 8s/step - loss: 0.2579 - accuracy: 0.9210 - val_loss: 13.0736 - val_accuracy: 0.1403\n",
      "Epoch 4/5\n",
      "126/127 [============================>.] - ETA: 7s - loss: 0.2113 - accuracy: 0.9361 \n",
      "Epoch 00004: val_accuracy did not improve from 0.14031\n",
      "127/127 [==============================] - 993s 8s/step - loss: 0.2106 - accuracy: 0.9361 - val_loss: 14.0744 - val_accuracy: 0.0991\n",
      "Epoch 5/5\n",
      "126/127 [============================>.] - ETA: 8s - loss: 0.2986 - accuracy: 0.9208 \n",
      "Epoch 00005: val_accuracy did not improve from 0.14031\n",
      "127/127 [==============================] - 1090s 9s/step - loss: 0.2992 - accuracy: 0.9207 - val_loss: 13.7252 - val_accuracy: 0.1013\n",
      "CPU times: user 5h 56min 13s, sys: 5h 44min 14s, total: 11h 40min 27s\n",
      "Wall time: 1h 25min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e70265c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Re-train the model\n",
    "custom_InceptionResNetV2.fit_generator(train_generator.flow(X_tr, Y_tr, batch_size=batch_size), \n",
    "                                       epochs=epochs, \n",
    "                                       workers=4, \n",
    "                                       callbacks=callbacks,\n",
    "                                       validation_data=val_generator.flow(X_te, Y_te, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_InceptionResNetV2.save_weights(CUSTOM_INCEPTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values from the validation dataset\n",
    "Y_pred = custom_InceptionResNetV2.predict(X_te)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_te, axis=1) \n",
    "# compute the confusion matrix\n",
    "cnf_matrix = confusion_matrix(Y_true, Y_pred_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cnf_matrix, abbreviations=None, output=None ,size=12):\n",
    "    # plot the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(size, size))\n",
    "    ax = sns.heatmap(cnf_matrix, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor=\"gray\", fmt= '.1f',ax=ax)\n",
    "    if abbreviations:\n",
    "        ax.set_xticklabels(abbreviations)\n",
    "        ax.set_yticklabels(abbreviations)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    if output:\n",
    "        fig.savefig(output, dpi=300)\n",
    "\n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, abbreviations=ABBREVIATIONS, output='./fig/model2_CNN_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: \\t{accuracy_score(Y_true, Y_pred_classes):.3}')\n",
    "print(f'Precision: \\t{[round(k, 3) for k in precision_score(Y_true, Y_pred_classes, average=None)]}')\n",
    "print(f'Recall: \\t{[round(k, 3) for k in recall_score(Y_true, Y_pred_classes, average=None)]}')\n",
    "print(f'F1 score: \\t{[round(k, 3) for k in f1_score(Y_true, Y_pred_classes, average=None)]}')\n",
    "\n",
    "# Or all together....\n",
    "print(f'\\nclasification report:\\n {classification_report(Y_true, Y_pred_classes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2] *",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
